{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uHsNETiRPj4B"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xRCn8_gPj4D"
      },
      "source": [
        "# HW8.1 Text classification (sentiment analysis) with deep learning models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hm1tqYIpPj4M"
      },
      "source": [
        "In this homework, we will practice building various deep learning model architectures for text classification. We will be using the IMBD movie review data for the sentiment classification task. First we will load the data. Second, you will be able to use various model types we've learned so far to perform text classification.\n",
        "\n",
        "Remember to use GPUs for this one, otherwise it will be slow to train.\n",
        "\n",
        "Tips:\n",
        "- print out the data types and shapes after each step to verify you are doing what you expected to do.\n",
        "- when you build the model, pass in the validation data in `model.fit()` as an argument `validation_data=(X_val, Y_val)`. The test data should be reserved until after you trained the model and then you can test it with X_test and report the test accuracy. **Always report accuracy on Test data for each experiment you do and make the comparison based on Test data.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbClYHJCPj4O"
      },
      "source": [
        "## 1. Loading data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zOKJ5rePPj4P"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.layers import Conv1D\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import MaxPooling1D\n",
        "# fix random seed for reproducibility\n",
        "tf.random.set_seed(7)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syb8CTL8Pj4S"
      },
      "source": [
        "### Preprocessing data\n",
        "\n",
        "There are a few hyperparameters you have to set and then preprocess the text data accordingly. These hyperparameters are:\n",
        "\n",
        "1. `vocab_size` : if set to 5000, the model will only keep the number of top 5000 most frequent words in the vocablulary while processing the text. Any less frequent words are thrwon out.\n",
        "2. `max_review_length` : if set to 500, the model will keep the maxmium length of a review text at 500. Any reviews longer than 500 words will be truncated at 500 words, and any reviews shorter than 500 words will be padded with 0s or special padding token to match the 500 token length. This is to makes sure all input sentences during training in a batch have the same size, a requirement of the batched input.\n",
        "3. `embedding_vector_length` : this is the embedding vector you want to project each word too. For instance, we can set it to 256 or 512.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EETaH0QtPj4T",
        "outputId": "3b11afaa-a23c-4e6a-d2e0-adbc276a9c31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17464789/17464789 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "\n",
        "vocab_size = 5000\n",
        "max_review_length = 500\n",
        "\n",
        "# load the dataset but only keep the top n words with the argument num_words\n",
        "# please read documentation here: https://keras.io/api/datasets/imdb/\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=vocab_size)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Training dataset:', type(X_train))\n",
        "print('Training dataset label:', type(y_train))\n",
        "print('Dimension Train:', X_train.shape)\n",
        "print('Dimension Train label:', y_train.shape)\n",
        "print('Dimension Test:', X_test.shape)\n",
        "print('Dimension Test label:', y_test.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "trr2smBXR-B9",
        "outputId": "8bdac4fb-2234-4d1a-9cdc-174c01c6c047"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training dataset: <class 'numpy.ndarray'>\n",
            "Training dataset label: <class 'numpy.ndarray'>\n",
            "Dimension Train: (25000,)\n",
            "Dimension Train label: (25000,)\n",
            "Dimension Test: (25000,)\n",
            "Dimension Test label: (25000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdKmFfyxPj4U"
      },
      "source": [
        "### task 0: review length before and after padding\n",
        "\n",
        "print out the length of first 20 reviews below. Then execute the next cell to pad them and then print out again the lengths. what do you see?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1UxmQbvRPj4V",
        "outputId": "7f145048-56c1-4a39-d2eb-5ddcd4d4c593"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Review 1: Length = 218\n",
            "Review 2: Length = 189\n",
            "Review 3: Length = 141\n",
            "Review 4: Length = 550\n",
            "Review 5: Length = 147\n",
            "Review 6: Length = 43\n",
            "Review 7: Length = 123\n",
            "Review 8: Length = 562\n",
            "Review 9: Length = 233\n",
            "Review 10: Length = 130\n",
            "Review 11: Length = 450\n",
            "Review 12: Length = 99\n",
            "Review 13: Length = 117\n",
            "Review 14: Length = 238\n",
            "Review 15: Length = 109\n",
            "Review 16: Length = 129\n",
            "Review 17: Length = 163\n",
            "Review 18: Length = 752\n",
            "Review 19: Length = 212\n",
            "Review 20: Length = 177\n"
          ]
        }
      ],
      "source": [
        "# YOUR CODE HERE\n",
        "for i, review in enumerate(X_train[:20], start=1):\n",
        "    print(f\"Review {i}: Length = {len(review)}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, review in enumerate(X_test[:20], start=1):\n",
        "    print(f\"Review {i}: Length = {len(review)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ox0VP7D700H6",
        "outputId": "9a2b9f67-2f92-44b1-d125-0c352584b9a4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Review 1: Length = 68\n",
            "Review 2: Length = 260\n",
            "Review 3: Length = 603\n",
            "Review 4: Length = 181\n",
            "Review 5: Length = 108\n",
            "Review 6: Length = 132\n",
            "Review 7: Length = 761\n",
            "Review 8: Length = 180\n",
            "Review 9: Length = 134\n",
            "Review 10: Length = 370\n",
            "Review 11: Length = 209\n",
            "Review 12: Length = 248\n",
            "Review 13: Length = 398\n",
            "Review 14: Length = 326\n",
            "Review 15: Length = 131\n",
            "Review 16: Length = 255\n",
            "Review 17: Length = 127\n",
            "Review 18: Length = 184\n",
            "Review 19: Length = 188\n",
            "Review 20: Length = 105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BIyaOEoWPj4W",
        "outputId": "3532ff46-367d-4013-e2fc-78a7cc310606"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Review 1: Length = 500\n",
            "Train Review 2: Length = 500\n",
            "Train Review 3: Length = 500\n",
            "Train Review 4: Length = 500\n",
            "Train Review 5: Length = 500\n",
            "Train Review 6: Length = 500\n",
            "Train Review 7: Length = 500\n",
            "Train Review 8: Length = 500\n",
            "Train Review 9: Length = 500\n",
            "Train Review 10: Length = 500\n",
            "Train Review 11: Length = 500\n",
            "Train Review 12: Length = 500\n",
            "Train Review 13: Length = 500\n",
            "Train Review 14: Length = 500\n",
            "Train Review 15: Length = 500\n",
            "Train Review 16: Length = 500\n",
            "Train Review 17: Length = 500\n",
            "Train Review 18: Length = 500\n",
            "Train Review 19: Length = 500\n",
            "Train Review 20: Length = 500\n",
            "Test Review 1: Length = 500\n",
            "Test Review 2: Length = 500\n",
            "Test Review 3: Length = 500\n",
            "Test Review 4: Length = 500\n",
            "Test Review 5: Length = 500\n",
            "Test Review 6: Length = 500\n",
            "Test Review 7: Length = 500\n",
            "Test Review 8: Length = 500\n",
            "Test Review 9: Length = 500\n",
            "Test Review 10: Length = 500\n",
            "Test Review 11: Length = 500\n",
            "Test Review 12: Length = 500\n",
            "Test Review 13: Length = 500\n",
            "Test Review 14: Length = 500\n",
            "Test Review 15: Length = 500\n",
            "Test Review 16: Length = 500\n",
            "Test Review 17: Length = 500\n",
            "Test Review 18: Length = 500\n",
            "Test Review 19: Length = 500\n",
            "Test Review 20: Length = 500\n"
          ]
        }
      ],
      "source": [
        "# truncate and pad input sequences\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
        "\n",
        "for i, review in enumerate(X_train[:20], start=1):\n",
        "    print(f\"Train Review {i}: Length = {len(review)}\")\n",
        "\n",
        "for i, review in enumerate(X_test[:20], start=1):\n",
        "    print(f\"Test Review {i}: Length = {len(review)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I observed that after truncating and padding, the review length for all the for all the first 20 review length is now 500, as opposed to when they initially had their initial individual actual review length. Review lengths initially more than 500 was truncated to fit the specified maximum length of 500, and review lengths initially less than 500 was padded with zeros to meet the desired sequence length of 500."
      ],
      "metadata": {
        "id": "uRj7qNk0zY3p"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqEeYpIRPj4X"
      },
      "source": [
        "### task 1: split training data and label into train and validation sets, X_train and X_val.\n",
        "\n",
        "Use 20% of your training data for validation, and the rest 80% for training. Then, also pad your new training data and validation data the same way as you did for train and test. You can leave the Test data alone and reserve that portion for testing only after training the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "3hT5T1EwPj4Y"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_val, Y_train, Y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=25)\n",
        "\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=max_review_length)\n",
        "\n",
        "x_val = sequence.pad_sequences(x_val, maxlen=max_review_length)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, review in enumerate(x_train[:20], start=1):\n",
        "    print(f\"New_Train Review {i}: Length = {len(review)}\")\n",
        "\n",
        "for i, review in enumerate(x_val[:20], start=1):\n",
        "    print(f\"Val Review {i}: Length = {len(review)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6q1_-liV2o5W",
        "outputId": "81dd8c62-9a04-4c74-ccad-31036af11404"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New_Train Review 1: Length = 500\n",
            "New_Train Review 2: Length = 500\n",
            "New_Train Review 3: Length = 500\n",
            "New_Train Review 4: Length = 500\n",
            "New_Train Review 5: Length = 500\n",
            "New_Train Review 6: Length = 500\n",
            "New_Train Review 7: Length = 500\n",
            "New_Train Review 8: Length = 500\n",
            "New_Train Review 9: Length = 500\n",
            "New_Train Review 10: Length = 500\n",
            "New_Train Review 11: Length = 500\n",
            "New_Train Review 12: Length = 500\n",
            "New_Train Review 13: Length = 500\n",
            "New_Train Review 14: Length = 500\n",
            "New_Train Review 15: Length = 500\n",
            "New_Train Review 16: Length = 500\n",
            "New_Train Review 17: Length = 500\n",
            "New_Train Review 18: Length = 500\n",
            "New_Train Review 19: Length = 500\n",
            "New_Train Review 20: Length = 500\n",
            "Val Review 1: Length = 500\n",
            "Val Review 2: Length = 500\n",
            "Val Review 3: Length = 500\n",
            "Val Review 4: Length = 500\n",
            "Val Review 5: Length = 500\n",
            "Val Review 6: Length = 500\n",
            "Val Review 7: Length = 500\n",
            "Val Review 8: Length = 500\n",
            "Val Review 9: Length = 500\n",
            "Val Review 10: Length = 500\n",
            "Val Review 11: Length = 500\n",
            "Val Review 12: Length = 500\n",
            "Val Review 13: Length = 500\n",
            "Val Review 14: Length = 500\n",
            "Val Review 15: Length = 500\n",
            "Val Review 16: Length = 500\n",
            "Val Review 17: Length = 500\n",
            "Val Review 18: Length = 500\n",
            "Val Review 19: Length = 500\n",
            "Val Review 20: Length = 500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9d2Is7UPj4Z"
      },
      "source": [
        "### Understanding the text data format\n",
        "\n",
        "The text data for deep learning is represented in a way that each word is mapped into a integer index first, 1, 2, 3, ..., N, assuming the text has N words in the vocabulary. Then we construct a giant embedding matrix where each word has an entry. At training time, you just use the index to retrieve the word embedding correspond to that entry (such as the k-th embedding) from this matrix. Let's inspect the input data representation before it is projected into the embedding space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUWFw4s3Pj4a",
        "outputId": "eb9213fb-6cb8-40be-e5bb-671bc88be331"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(20000, 500) (20000,) train sequences\n",
            "(5000, 500) (5000,) val sequences\n",
            "(25000, 500) (25000,) test sequences\n"
          ]
        }
      ],
      "source": [
        "print(x_train.shape, Y_train.shape, 'train sequences')\n",
        "print(x_val.shape, Y_val.shape,'val sequences')\n",
        "print(X_test.shape, y_test.shape, 'test sequences')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dVKY385pPj4a",
        "outputId": "1bdabb3f-fe25-4742-8dfa-762d8a96240c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[   2   19    4    2   14  390   62  434  967   17 1732   94 1573    2\n",
            " 2775  684   24    2  101    2   33    2  130   42  127   12 4562  125\n",
            "   19    4 1295   20  812   57 4148  807   21  260  621    2   11   14\n",
            "  420   45   23    4  481 1406   24    2    4  395    7    2   42 3654\n",
            "   34   35   23 1682    2   14  173    7    4  390  367   19    4  277\n",
            " 2424   82   93 3697  361 1604  425 4320   43   40    4  154   58  102\n",
            "  137  134 2818   26    6  227 3694    5 3701   36   26   93   38 4645\n",
            "    5   19   35    2   18  247   74  101    2   18    4 1295   22   10\n",
            "   10 2496  212   22  167 3766 4543    2   16    4  132   11    4 3039\n",
            "   18   14  320  534 3668   29   69   77    4  167    7  111    7    4\n",
            "  833 1290    7   32   58 1890   84   40    4    2 1094 1992 1148    2\n",
            "    2 3414    5 1738    2   29   16   57 3128    8    8  248   17   29\n",
            "   69  224    6  176    7  157   23  699  201   10   10   12  152  977\n",
            "   15   29    5  443 2065   69  126  952  295  159   17   13  566  169\n",
            "  101 2314    7   14  758   21 4235   34    4 3746    7    4   22   36\n",
            " 4102   11  399   38   19 1549 2511  259   37  526 2065   16 1887   15\n",
            "    2   16   82    6  478  212  167   17   73   17    6  212 1799   29\n",
            "   16   43   17 3969  496    4  370   17   29   16   11 1011    7   12\n",
            "   68  346    2  215   28   77    6    2   31   19    2    5    2   44\n",
            "   89    8   81  183   12    9 1734   15  111    7    4 1994   71    2\n",
            "    2   39   27  205 1295  431  504   18  463    4 3288    7 1489    4\n",
            " 2162    7 4518   23   19    2 3006 4601    2   16  224   34 2065    5\n",
            "    2    2    2   11   31    7    4    2  241    2    4    2    2   15\n",
            "   16    6  788  463    7   27 3914   11    6    2   10   10    2  697\n",
            "   15   75   22  263  182   19    6  370    2   12    6  107 2044 1459\n",
            "   14   31  192    9   33    4 1324    7   38  111    7 1994   12    9\n",
            "    6    2 2673   18   27   22  231   10   10    4  177   16  392    5\n",
            "  280  174   43    2  368    7 2496  676 3006 4601   16    2    5 3009\n",
            "   17  443    2  730  132 3509  428    4  154    2    2    2 2178    4\n",
            " 4290   12 2022   15    2    4    2    2    2    2    2 2496    7    2\n",
            "  346  212  201   16    4    2 1002  592    2  742  963 1654 1335    2\n",
            " 3864 2238    5    2    2   32    2   46   14 2256 1295  177   10   10\n",
            "  209    2    4    2  903   43  135   15  422   50    9  242    6 2037\n",
            "    8   30 2051  133   48   24    4   31  460 1046    4    2  210  272\n",
            "    2   23    4   85  499    7    4    2   95   89   44   30 4690   11\n",
            "   51   25  942   18   88   25   43  203   79   12]\n",
            "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    1   13   28    8  135   15   14   16\n",
            "   24   55 1127   21  740   44 1597    5  916  771    8 3330    6  736\n",
            "    6  117   99  606 2406 2266 2641 3338 1077  899  198  777   13   66\n",
            "  423   44   15  173    7    4   20   10   10    2    2    2    9  190\n",
            "  576  642    5 2931    2 2431    2  468   66    2   29  144   28   77\n",
            "   53 1612   19   41   18   89   59   47 1899   90   10   10 2659   14\n",
            "   20    9 3075    5 1597   11    2    2   13  386   14   20   43   18\n",
            " 1506  253    2 2406   43   18  253   53 1629 2545  102   34 2641 3338\n",
            "    2  377  442 1575   52   33   14    2 1255  157]\n"
          ]
        }
      ],
      "source": [
        "# inspect the 24th review:\n",
        "print(x_train[23])\n",
        "\n",
        "# inspect the 2nd review:\n",
        "print(x_train[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAm815jiPj4b"
      },
      "source": [
        "### Q1: Do you see the difference between these two examples in terms of padding? explain what is happening."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# YOUR ANSWER HERE\n",
        "Yes I can see the difference. Here, the 24th review has the max review length of words which is 500 (it could be more, and was truncated to 500), hence no need for padding this particular review. However, the second review has fewer words than the max review length, hence the first entries were padded with zeros before the actual first word entry. This is to ensure that all sequences have thesame length.\n"
      ],
      "metadata": {
        "id": "xiFDqZBC6wOF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "0wxRXAssPj4c",
        "outputId": "83e2c958-b030-48ef-ca48-f01b80d7255b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "1641221/1641221 [==============================] - 0s 0us/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"story is also true to life if only it was developed a little better i felt that the more promising stories in this <UNK> <UNK> were told from the male point of view which is fine but it brings down the emotional <UNK> of these stories because the female characters <UNK> <UNK> and <UNK> <UNK> in particular have all the depth of a half filled bath <UNK> wasn't this film supposed to be about <UNK> different <UNK> now the actors john <UNK> still needs to learn acting while <UNK> <UNK> is <UNK> and endearing as ever <UNK> kapoor gets a role written just for him but sometimes <UNK> the boredom of his character since she didn't get a <UNK> character to portray <UNK> <UNK> uses her charming smile and natural acting style to cover up for it <UNK> <UNK> is fine despite going a <UNK> over the top in a few scenes <UNK> <UNK> has nothing much to do but she does remind us that she's the same girl who surprised us with her <UNK> performance in <UNK> <UNK> tries to make up for that huge mistake called <UNK> <UNK> and succeeds to a large extent br br that brings me the most irritating track of the film which <UNK> ends up <UNK> the <UNK> screen time <UNK> khan and <UNK> <UNK> the track is irritating mainly because of them but i must credit them for their <UNK> they're consistently bad all through the film <UNK> could well be the next queen of <UNK> i don't buy the crap that her role that of an <UNK> <UNK> required her to act over the top somebody should tell her the difference between being <UNK> because the character demands it and downright <UNK> if you've seen <UNK> <UNK> who seems to be the inspiration behind this role in her interviews and <UNK> boss you'll know what i mean i strongly feel that if <UNK> <UNK> had taken <UNK> <UNK> in this role rather than a bigger star like <UNK> the story would have worked better <UNK> <UNK> phony accent is well <UNK> <UNK> <UNK> is less <UNK> than <UNK> br br to be fair to the director he does manage to add some good directorial touches to the film i particularly liked the use of grey as the <UNK> color in the <UNK> kapoor <UNK> <UNK> story as a <UNK> for their boring existence and the bright colors that come into the story with the arrival of the other woman but will anyone choose to paint their house in <UNK> depressing <UNK> of grey that's acceptable cinematic <UNK> i would say but when the film runs almost for 4 hours it almost feels like the director is trying too hard to give the audience a glimpse of his <UNK> in an <UNK> <UNK> montage of <UNK> sequences br br maybe <UNK> <UNK> wants us to sit in the theatre for as long as is <UNK> possible <UNK> <UNK> <UNK> ho <UNK> ho\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "# now let's look at these reviews in actual words\n",
        "\n",
        "INDEX_FROM = 3\n",
        "word_index = imdb.get_word_index()\n",
        "word_index = {key:(value+INDEX_FROM) for key,value in word_index.items()}\n",
        "word_index[\"<PAD>\"] = 0    # the padding token\n",
        "word_index[\"<START>\"] = 1  # the starting token\n",
        "word_index[\"<UNK>\"] = 2    # the unknown token\n",
        "reverse_word_index = {value:key for key, value in word_index.items()}\n",
        "\n",
        "def decode_review(text):\n",
        "    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n",
        "\n",
        "decode_review(X_train[23])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-p7gFXIPj4d"
      },
      "source": [
        "## 2. Build models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7JhPlqbPj4d"
      },
      "source": [
        "In this homework you will demonstrate your ability to build various kinds of models for sequence (text) classification. Specifically:\n",
        "\n",
        "- Using single architectures:\n",
        "  - CNN 1d layer (https://keras.io/api/layers/convolution_layers/convolution1d/)\n",
        "  - LSTM (https://keras.io/api/layers/recurrent_layers/lstm/)\n",
        "  - Bidirectional LSTM (https://keras.io/api/layers/recurrent_layers/bidirectional/) (for this one, you want to do something like `Bidirectional(LSTM(num_units))`)\n",
        "\n",
        "- Stacking these layers together: Conv-LSTM, Conv-BiLSTM: it just means once you have your conv1d layers, add another (or several) LSTM or Bidirectional LSTM on top of it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xiEnrl5OPj4d"
      },
      "source": [
        "#### 2.1 Conv1d\n",
        "\n",
        "Tips: to get started, first build a Sequential model. Then add a `Embedding` layer (https://keras.io/api/layers/core_layers/embedding/) with the `input_dim` equal to your vocab_size, and the `output_dim` equal to your `embedding_vector_length`. You should also add an argument `input_length` being equal to your `max_review_length`. Then add a conv1d layer with multiple filters (maybe 64), then a `MaxPooling1D` layer with a pooling factor of 2. You can feel free to repeat this structure another 1 to 3 times if you want. Then before you go into the Dense layer, you need to `Flatten` the output from Conv layers. Once you flattened the output, you can add either another (nonlinear) Dense layer with some units (such as 128) or not, before you add the final Dense layer with a sigmoid activation.\n",
        "\n",
        "Overall the flow is:\n",
        "\n",
        "Embedding -> (Conv1D->MaxPooling1D) * K times -> Flatten -> (Dense with relu activation) * M times -> output Dense layer with sigmoid activation.\n",
        "\n",
        "Note that K>=1 but M>=0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LXRz9aIsPj4e",
        "outputId": "0e37545e-648e-401c-c278-2f5b990486f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 500, 256)          1280000   \n",
            "                                                                 \n",
            " conv1d (Conv1D)             (None, 498, 64)           49216     \n",
            "                                                                 \n",
            " max_pooling1d (MaxPooling1  (None, 249, 64)           0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " conv1d_1 (Conv1D)           (None, 247, 64)           12352     \n",
            "                                                                 \n",
            " max_pooling1d_1 (MaxPoolin  (None, 123, 64)           0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_2 (Conv1D)           (None, 121, 64)           12352     \n",
            "                                                                 \n",
            " max_pooling1d_2 (MaxPoolin  (None, 60, 64)            0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 3840)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 256)               983296    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 257       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2337473 (8.92 MB)\n",
            "Trainable params: 2337473 (8.92 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# task 2:CNN\n",
        "from tensorflow.keras.layers import Flatten\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(5000, 256, input_length=max_review_length))\n",
        "model.add(Conv1D(64, 3, activation='relu'))\n",
        "model.add(MaxPooling1D(2))\n",
        "model.add(Conv1D(64, 3, activation='relu'))\n",
        "model.add(MaxPooling1D(2))\n",
        "model.add(Conv1D(64, 3, activation='relu'))\n",
        "model.add(MaxPooling1D(2))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# see the model status now\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    optimizer= 'adam',\n",
        "    loss= 'binary_crossentropy',\n",
        "    metrics=['accuracy'],\n",
        ")\n",
        "\n",
        "#model summary\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jjr2zdM1slnu",
        "outputId": "3da674d0-daeb-4cdd-d3ad-8814f6f3b5d9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 500, 256)          1280000   \n",
            "                                                                 \n",
            " conv1d (Conv1D)             (None, 498, 64)           49216     \n",
            "                                                                 \n",
            " max_pooling1d (MaxPooling1  (None, 249, 64)           0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " conv1d_1 (Conv1D)           (None, 247, 64)           12352     \n",
            "                                                                 \n",
            " max_pooling1d_1 (MaxPoolin  (None, 123, 64)           0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_2 (Conv1D)           (None, 121, 64)           12352     \n",
            "                                                                 \n",
            " max_pooling1d_2 (MaxPoolin  (None, 60, 64)            0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 3840)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 256)               983296    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 257       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2337473 (8.92 MB)\n",
            "Trainable params: 2337473 (8.92 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "callback = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor=\"loss\",\n",
        "    patience=3,\n",
        "    mode=\"auto\",\n",
        "    start_from_epoch=2,\n",
        ")"
      ],
      "metadata": {
        "id": "ksJkXavwyJ9R"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "his = model.fit(x_train, Y_train, epochs=10, batch_size=128, callbacks=[callback],\n",
        "validation_data=(x_val, Y_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-BUjMMW3vAOD",
        "outputId": "4782e75b-5094-48cf-e359-a8bd6f867f6f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "157/157 [==============================] - 22s 102ms/step - loss: 0.4747 - accuracy: 0.7354 - val_loss: 0.3004 - val_accuracy: 0.8742\n",
            "Epoch 2/10\n",
            "157/157 [==============================] - 11s 67ms/step - loss: 0.2312 - accuracy: 0.9075 - val_loss: 0.2870 - val_accuracy: 0.8864\n",
            "Epoch 3/10\n",
            "157/157 [==============================] - 8s 51ms/step - loss: 0.1348 - accuracy: 0.9495 - val_loss: 0.3216 - val_accuracy: 0.8812\n",
            "Epoch 4/10\n",
            "157/157 [==============================] - 6s 38ms/step - loss: 0.0636 - accuracy: 0.9776 - val_loss: 0.4613 - val_accuracy: 0.8746\n",
            "Epoch 5/10\n",
            "157/157 [==============================] - 5s 34ms/step - loss: 0.0362 - accuracy: 0.9877 - val_loss: 0.8653 - val_accuracy: 0.8406\n",
            "Epoch 6/10\n",
            "157/157 [==============================] - 5s 32ms/step - loss: 0.0277 - accuracy: 0.9909 - val_loss: 0.6859 - val_accuracy: 0.8778\n",
            "Epoch 7/10\n",
            "157/157 [==============================] - 4s 28ms/step - loss: 0.0199 - accuracy: 0.9936 - val_loss: 0.7495 - val_accuracy: 0.8768\n",
            "Epoch 8/10\n",
            "157/157 [==============================] - 4s 24ms/step - loss: 0.0074 - accuracy: 0.9977 - val_loss: 0.9383 - val_accuracy: 0.8650\n",
            "Epoch 9/10\n",
            "157/157 [==============================] - 4s 24ms/step - loss: 0.0301 - accuracy: 0.9898 - val_loss: 0.6417 - val_accuracy: 0.8652\n",
            "Epoch 10/10\n",
            "157/157 [==============================] - 3s 21ms/step - loss: 0.0202 - accuracy: 0.9934 - val_loss: 0.7749 - val_accuracy: 0.8748\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#evaluate on test set\n",
        "evaluation = model.evaluate(X_test, y_test)\n",
        "for i in range(len(evaluation)):\n",
        "  print(f'{model.metrics_names[i]} ---> {evaluation[i]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZKDl8Vv30D1",
        "outputId": "cc9f2e6f-aab8-45aa-ff5b-32a458649fad"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 3s 3ms/step - loss: 0.8265 - accuracy: 0.8612\n",
            "loss ---> 0.8265305757522583\n",
            "accuracy ---> 0.861240029335022\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIO7CPLbPj4e"
      },
      "source": [
        "#### 2.2 LSTM and Bi-LSTM\n",
        "\n",
        "Once you built the network with CNN, this will be easy. Simply replace the Conv1d layer with the LSTM layer and the Bidirectional LSTM layer (read the documentations linked above).Try using 64 units for the LSTM layer and try 128 as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I40EkAyXPj4f",
        "outputId": "4538a349-0a53-41a4-e891-e920b20e0ffa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_9 (Embedding)     (None, 500, 256)          1280000   \n",
            "                                                                 \n",
            " lstm_7 (LSTM)               (None, 64)                82176     \n",
            "                                                                 \n",
            " flatten_6 (Flatten)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 256)               16640     \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 1)                 257       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1379073 (5.26 MB)\n",
            "Trainable params: 1379073 (5.26 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# task 3: LSTM and Bi-LSTM\n",
        "model_lstm = Sequential()\n",
        "model_lstm.add(Embedding(5000, 256, input_length=max_review_length))\n",
        "#add lstm layer with 64 units\n",
        "model_lstm.add(LSTM(64, activation='tanh'))\n",
        "\n",
        "model_lstm.add(Flatten())\n",
        "model_lstm.add(Dense(256))\n",
        "model_lstm.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "#model summary\n",
        "model_lstm.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#compile the model\n",
        "model_lstm.compile(\n",
        "    optimizer= 'adam',\n",
        "    loss= 'binary_crossentropy',\n",
        "    metrics=['accuracy'],\n",
        ")\n",
        "\n",
        "#model training\n",
        "his_lstm = model_lstm.fit(x_train, Y_train, epochs=10, batch_size=128, callbacks=[callback],\n",
        "validation_data=(x_val, Y_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g5Rulcno-nkM",
        "outputId": "fec85b0c-71b1-4c96-bab3-008009b974c8"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "157/157 [==============================] - 21s 117ms/step - loss: 0.4199 - accuracy: 0.7970 - val_loss: 0.3285 - val_accuracy: 0.8684\n",
            "Epoch 2/10\n",
            "157/157 [==============================] - 12s 79ms/step - loss: 0.2430 - accuracy: 0.9018 - val_loss: 0.3461 - val_accuracy: 0.8632\n",
            "Epoch 3/10\n",
            "157/157 [==============================] - 11s 71ms/step - loss: 0.1903 - accuracy: 0.9268 - val_loss: 0.3350 - val_accuracy: 0.8684\n",
            "Epoch 4/10\n",
            "157/157 [==============================] - 10s 62ms/step - loss: 0.1383 - accuracy: 0.9493 - val_loss: 0.3959 - val_accuracy: 0.8476\n",
            "Epoch 5/10\n",
            "157/157 [==============================] - 7s 45ms/step - loss: 0.1186 - accuracy: 0.9568 - val_loss: 0.4217 - val_accuracy: 0.8516\n",
            "Epoch 6/10\n",
            "157/157 [==============================] - 8s 49ms/step - loss: 0.1046 - accuracy: 0.9625 - val_loss: 0.5021 - val_accuracy: 0.8548\n",
            "Epoch 7/10\n",
            "157/157 [==============================] - 6s 41ms/step - loss: 0.0907 - accuracy: 0.9676 - val_loss: 0.6138 - val_accuracy: 0.8614\n",
            "Epoch 8/10\n",
            "157/157 [==============================] - 6s 38ms/step - loss: 0.0624 - accuracy: 0.9797 - val_loss: 0.6009 - val_accuracy: 0.8398\n",
            "Epoch 9/10\n",
            "157/157 [==============================] - 6s 38ms/step - loss: 0.0700 - accuracy: 0.9754 - val_loss: 0.6568 - val_accuracy: 0.8570\n",
            "Epoch 10/10\n",
            "157/157 [==============================] - 5s 32ms/step - loss: 0.0470 - accuracy: 0.9846 - val_loss: 0.6981 - val_accuracy: 0.8430\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#evaluate on test set\n",
        "evaluation = model_lstm.evaluate(X_test, y_test)\n",
        "for i in range(len(evaluation)):\n",
        "  print(f'{model.metrics_names[i]} ---> {evaluation[i]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdzyeERyKuHl",
        "outputId": "cbaae892-63a5-44f5-8c6f-cc644699aefa"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 7s 9ms/step - loss: 0.7022 - accuracy: 0.8384\n",
            "loss ---> 0.7022024393081665\n",
            "accuracy ---> 0.8384400010108948\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test for the error I asked you about after class\n",
        "\n",
        "model_test = Sequential()\n",
        "model_test.add(Embedding(5000, 256, input_length=max_review_length))\n",
        "#add lstm layer with 64 units\n",
        "model_test.add(LSTM(64, activation='tanh'))\n",
        "model_test.add(LSTM(128, return_sequences=True))\n",
        "\n",
        "model_test.add(Flatten())\n",
        "model_test.add(Dense(256))\n",
        "model_test.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "#model summary\n",
        "model_test.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "DLCRQCP45T5U",
        "outputId": "bcc0927a-e18a-4e3e-8859-b3d76d0d7ac9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Input 0 of layer \"lstm_5\" is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 64)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-3f58c42a6eb3>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#add lstm layer with 64 units\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmodel_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tanh'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmodel_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mmodel_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFlatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/trackable/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mndim\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    236\u001b[0m                     \u001b[0;34mf'Input {input_index} of layer \"{layer_name}\" '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m                     \u001b[0;34m\"is incompatible with the layer: \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input 0 of layer \"lstm_5\" is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 64)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# task 3: LSTM and Bi-LSTM\n",
        "model_lstm2 = Sequential()\n",
        "model_lstm2.add(Embedding(5000, 512, input_length=max_review_length))\n",
        "#add lstm layer with 128 units\n",
        "model_lstm2.add(LSTM(128, activation='tanh'))\n",
        "\n",
        "model_lstm2.add(Flatten())\n",
        "model_lstm2.add(Dense(256))\n",
        "model_lstm2.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "#model summary\n",
        "model_lstm2.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7JAe3LTu-9wA",
        "outputId": "605d3b33-e68d-45b9-ed3b-10502b93ed1f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_2 (Embedding)     (None, 500, 512)          2560000   \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 128)               328192    \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 256)               33024     \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 1)                 257       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2921473 (11.14 MB)\n",
            "Trainable params: 2921473 (11.14 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#compile the model\n",
        "model_lstm2.compile(\n",
        "    optimizer= 'adam',\n",
        "    loss= 'binary_crossentropy',\n",
        "    metrics=['accuracy'],\n",
        ")\n",
        "\n",
        "#model training\n",
        "his_lstm2 = model_lstm2.fit(x_train, Y_train, epochs=10, batch_size=64, callbacks=[callback],\n",
        "validation_data=(x_val, Y_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kdeP71DFMTae",
        "outputId": "6264b62f-c024-491a-f3d4-ed047337a40f"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "313/313 [==============================] - 37s 110ms/step - loss: 0.4422 - accuracy: 0.7875 - val_loss: 0.3486 - val_accuracy: 0.8666\n",
            "Epoch 2/10\n",
            "313/313 [==============================] - 23s 74ms/step - loss: 0.3265 - accuracy: 0.8607 - val_loss: 0.3129 - val_accuracy: 0.8774\n",
            "Epoch 3/10\n",
            "313/313 [==============================] - 16s 52ms/step - loss: 0.2100 - accuracy: 0.9170 - val_loss: 0.3436 - val_accuracy: 0.8516\n",
            "Epoch 4/10\n",
            "313/313 [==============================] - 16s 51ms/step - loss: 0.1751 - accuracy: 0.9319 - val_loss: 0.4173 - val_accuracy: 0.8584\n",
            "Epoch 5/10\n",
            "313/313 [==============================] - 14s 43ms/step - loss: 0.1316 - accuracy: 0.9516 - val_loss: 0.5635 - val_accuracy: 0.7866\n",
            "Epoch 6/10\n",
            "313/313 [==============================] - 15s 47ms/step - loss: 0.1313 - accuracy: 0.9508 - val_loss: 0.5648 - val_accuracy: 0.8586\n",
            "Epoch 7/10\n",
            "313/313 [==============================] - 14s 45ms/step - loss: 0.0881 - accuracy: 0.9689 - val_loss: 0.5477 - val_accuracy: 0.8542\n",
            "Epoch 8/10\n",
            "313/313 [==============================] - 13s 42ms/step - loss: 0.0606 - accuracy: 0.9793 - val_loss: 0.6458 - val_accuracy: 0.8512\n",
            "Epoch 9/10\n",
            "313/313 [==============================] - 13s 43ms/step - loss: 0.1090 - accuracy: 0.9623 - val_loss: 0.6300 - val_accuracy: 0.8532\n",
            "Epoch 10/10\n",
            "313/313 [==============================] - 13s 41ms/step - loss: 0.0478 - accuracy: 0.9836 - val_loss: 0.6455 - val_accuracy: 0.8540\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#evaluate on test set\n",
        "evaluation = model_lstm2.evaluate(X_test, y_test)\n",
        "for i in range(len(evaluation)):\n",
        "  print(f'{model.metrics_names[i]} ---> {evaluation[i]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nYSa8c6Rr7jK",
        "outputId": "1a196d05-bb9c-41a0-cd91-c8a8dd7972f8"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 8s 10ms/step - loss: 0.6368 - accuracy: 0.8531\n",
            "loss ---> 0.6367759108543396\n",
            "accuracy ---> 0.8531200289726257\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# task 3: LSTM and Bi-LSTM\n",
        "from keras.layers import Bidirectional\n",
        "\n",
        "model_bilstm = Sequential()\n",
        "model_bilstm.add(Embedding(5000, 256, input_length=max_review_length))\n",
        "#add bilstm layer with 64 units\n",
        "model_bilstm.add(Bidirectional(LSTM(64)))\n",
        "\n",
        "model_bilstm.add(Flatten())\n",
        "model_bilstm.add(Dense(256))\n",
        "model_bilstm.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "#model summary\n",
        "model_bilstm.summary()"
      ],
      "metadata": {
        "id": "BV-c9d89XBkQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de27f927-9e44-4c7a-9ccf-4c61d6e9fa7a"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_6 (Embedding)     (None, 500, 256)          1280000   \n",
            "                                                                 \n",
            " bidirectional (Bidirection  (None, 128)               164352    \n",
            " al)                                                             \n",
            "                                                                 \n",
            " flatten_3 (Flatten)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 256)               33024     \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 1)                 257       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1477633 (5.64 MB)\n",
            "Trainable params: 1477633 (5.64 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#compile the model\n",
        "model_bilstm.compile(\n",
        "    optimizer= 'adam',\n",
        "    loss= 'binary_crossentropy',\n",
        "    metrics=['accuracy'],\n",
        ")\n",
        "\n",
        "#model training\n",
        "his_lstm2 = model_bilstm.fit(x_train, Y_train, epochs=15, batch_size=128, callbacks=[callback],\n",
        "validation_data=(x_val, Y_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hh5A1DhxtggT",
        "outputId": "7f9e4db3-fb20-402b-af0b-2fa8106f7428"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "157/157 [==============================] - 26s 144ms/step - loss: 0.4085 - accuracy: 0.8002 - val_loss: 0.3158 - val_accuracy: 0.8722\n",
            "Epoch 2/15\n",
            "157/157 [==============================] - 16s 102ms/step - loss: 0.2320 - accuracy: 0.9072 - val_loss: 0.3108 - val_accuracy: 0.8732\n",
            "Epoch 3/15\n",
            "157/157 [==============================] - 13s 85ms/step - loss: 0.1888 - accuracy: 0.9277 - val_loss: 0.3602 - val_accuracy: 0.8536\n",
            "Epoch 4/15\n",
            "157/157 [==============================] - 13s 80ms/step - loss: 0.1328 - accuracy: 0.9492 - val_loss: 0.4266 - val_accuracy: 0.8328\n",
            "Epoch 5/15\n",
            "157/157 [==============================] - 11s 71ms/step - loss: 0.1027 - accuracy: 0.9616 - val_loss: 0.4727 - val_accuracy: 0.8576\n",
            "Epoch 6/15\n",
            "157/157 [==============================] - 11s 73ms/step - loss: 0.0881 - accuracy: 0.9693 - val_loss: 0.5058 - val_accuracy: 0.8588\n",
            "Epoch 7/15\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.0755 - accuracy: 0.9730 - val_loss: 0.5740 - val_accuracy: 0.8578\n",
            "Epoch 8/15\n",
            "157/157 [==============================] - 10s 64ms/step - loss: 0.0389 - accuracy: 0.9883 - val_loss: 0.8250 - val_accuracy: 0.8392\n",
            "Epoch 9/15\n",
            "157/157 [==============================] - 10s 63ms/step - loss: 0.0370 - accuracy: 0.9874 - val_loss: 0.8417 - val_accuracy: 0.8354\n",
            "Epoch 10/15\n",
            "157/157 [==============================] - 9s 58ms/step - loss: 0.0525 - accuracy: 0.9819 - val_loss: 0.6608 - val_accuracy: 0.8494\n",
            "Epoch 11/15\n",
            "157/157 [==============================] - 10s 61ms/step - loss: 0.0291 - accuracy: 0.9910 - val_loss: 0.8392 - val_accuracy: 0.8534\n",
            "Epoch 12/15\n",
            "157/157 [==============================] - 9s 59ms/step - loss: 0.0302 - accuracy: 0.9904 - val_loss: 0.7110 - val_accuracy: 0.8458\n",
            "Epoch 13/15\n",
            "157/157 [==============================] - 10s 62ms/step - loss: 0.0174 - accuracy: 0.9944 - val_loss: 0.9557 - val_accuracy: 0.8568\n",
            "Epoch 14/15\n",
            "157/157 [==============================] - 9s 58ms/step - loss: 0.0147 - accuracy: 0.9954 - val_loss: 0.9960 - val_accuracy: 0.8184\n",
            "Epoch 15/15\n",
            "157/157 [==============================] - 9s 59ms/step - loss: 0.0346 - accuracy: 0.9877 - val_loss: 0.8947 - val_accuracy: 0.8502\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#evaluate on test set\n",
        "evaluation = model_bilstm.evaluate(X_test, y_test)\n",
        "for i in range(len(evaluation)):\n",
        "  print(f'{model.metrics_names[i]} ---> {evaluation[i]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "170Q8Ze0umYS",
        "outputId": "be877d13-9d1f-47d5-e367-24c1524ba0e1"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 12s 16ms/step - loss: 0.9017 - accuracy: 0.8439\n",
            "loss ---> 0.9016522765159607\n",
            "accuracy ---> 0.8438799977302551\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFaWaNI3Pj4f"
      },
      "source": [
        "#### 2.3 Conv-LSTM and Conv-BiLSTM\n",
        "\n",
        "In this last task, you will stack together the Conv1d layers and the LSTM layers. Try adding the LSTM layer after the Conv1d layers and see if it works. You can try using two Conv1d layers and then add a LSTM layers. Then replace the LSTM layer with Bi-LSTM layer. Try different combinations and see what's the best accuracy you can get on Test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "1SPxfvJlPj4g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c26d660-b2ae-4553-eee8-644b29c9b022"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_10 (Embedding)    (None, 500, 256)          1280000   \n",
            "                                                                 \n",
            " conv1d_3 (Conv1D)           (None, 498, 64)           49216     \n",
            "                                                                 \n",
            " lstm_8 (LSTM)               (None, 128)               98816     \n",
            "                                                                 \n",
            " flatten_7 (Flatten)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 128)               16512     \n",
            "                                                                 \n",
            " dense_15 (Dense)            (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1444673 (5.51 MB)\n",
            "Trainable params: 1444673 (5.51 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# task 3: Conv-LSTM\n",
        "model_clstm = Sequential()\n",
        "model_clstm.add(Embedding(5000, 256, input_length=max_review_length))\n",
        "model_clstm.add(Conv1D(64, 3, activation='relu'))\n",
        "model_clstm.add(LSTM(128, activation='tanh'))\n",
        "\n",
        "model_clstm.add(Flatten())\n",
        "model_clstm.add(Dense(128))\n",
        "model_clstm.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# see the model status now\n",
        "model_clstm.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#compile the model\n",
        "model_clstm.compile(\n",
        "    optimizer= 'adam',\n",
        "    loss= 'binary_crossentropy',\n",
        "    metrics=['accuracy'],\n",
        ")\n",
        "\n",
        "#model training\n",
        "his_clstm = model_clstm.fit(x_train, Y_train, epochs=10, batch_size=128, callbacks=[callback],\n",
        "validation_data=(x_val, Y_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ytXnd-X0x_7F",
        "outputId": "fbc8847b-bf51-4dc9-823c-60701273b9c5"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "157/157 [==============================] - 22s 122ms/step - loss: 0.4781 - accuracy: 0.7564 - val_loss: 0.3369 - val_accuracy: 0.8568\n",
            "Epoch 2/10\n",
            "157/157 [==============================] - 14s 86ms/step - loss: 0.2566 - accuracy: 0.8958 - val_loss: 0.3040 - val_accuracy: 0.8766\n",
            "Epoch 3/10\n",
            "157/157 [==============================] - 11s 71ms/step - loss: 0.1810 - accuracy: 0.9316 - val_loss: 0.3408 - val_accuracy: 0.8656\n",
            "Epoch 4/10\n",
            "157/157 [==============================] - 10s 63ms/step - loss: 0.1528 - accuracy: 0.9431 - val_loss: 0.3518 - val_accuracy: 0.8726\n",
            "Epoch 5/10\n",
            "157/157 [==============================] - 10s 60ms/step - loss: 0.0988 - accuracy: 0.9657 - val_loss: 0.4423 - val_accuracy: 0.8742\n",
            "Epoch 6/10\n",
            "157/157 [==============================] - 9s 58ms/step - loss: 0.0644 - accuracy: 0.9783 - val_loss: 0.5155 - val_accuracy: 0.8596\n",
            "Epoch 7/10\n",
            "157/157 [==============================] - 9s 54ms/step - loss: 0.0465 - accuracy: 0.9862 - val_loss: 0.6579 - val_accuracy: 0.8686\n",
            "Epoch 8/10\n",
            "157/157 [==============================] - 7s 48ms/step - loss: 0.0467 - accuracy: 0.9837 - val_loss: 0.6544 - val_accuracy: 0.8532\n",
            "Epoch 9/10\n",
            "157/157 [==============================] - 8s 50ms/step - loss: 0.0205 - accuracy: 0.9938 - val_loss: 0.8257 - val_accuracy: 0.8602\n",
            "Epoch 10/10\n",
            "157/157 [==============================] - 7s 47ms/step - loss: 0.0065 - accuracy: 0.9984 - val_loss: 0.8766 - val_accuracy: 0.8594\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#evaluate on test set\n",
        "evaluation = model_clstm.evaluate(X_test, y_test)\n",
        "for i in range(len(evaluation)):\n",
        "  print(f'{model.metrics_names[i]} ---> {evaluation[i]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4t1i6SevyqQu",
        "outputId": "0cfc7b8a-d0e8-4d4c-9433-ff97995b2fe1"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 7s 9ms/step - loss: 0.8755 - accuracy: 0.8568\n",
            "loss ---> 0.8754912614822388\n",
            "accuracy ---> 0.8568400144577026\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#stacking 3 Conv1D layers and one LSTM layer\n",
        "model_clstm2 = Sequential()\n",
        "model_clstm2.add(Embedding(5000, 256, input_length=max_review_length))\n",
        "model_clstm2.add(Conv1D(64, 3, activation='relu'))\n",
        "model_clstm2.add(MaxPooling1D(2))\n",
        "model_clstm2.add(Conv1D(64, 3, activation='relu'))\n",
        "model_clstm2.add(MaxPooling1D(2))\n",
        "model_clstm2.add(Conv1D(128, 3, activation='relu'))\n",
        "model_clstm2.add(MaxPooling1D(2))\n",
        "model_clstm2.add(LSTM(128, activation='tanh'))\n",
        "\n",
        "model_clstm2.add(Flatten())\n",
        "model_clstm2.add(Dense(256))\n",
        "model_clstm2.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# see the model status now\n",
        "model_clstm2.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GfCUq3kQy1Od",
        "outputId": "bd52a220-1416-44d4-dac1-79c0420dd00f"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_11\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_11 (Embedding)    (None, 500, 256)          1280000   \n",
            "                                                                 \n",
            " conv1d_4 (Conv1D)           (None, 498, 64)           49216     \n",
            "                                                                 \n",
            " max_pooling1d_3 (MaxPoolin  (None, 249, 64)           0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_5 (Conv1D)           (None, 247, 64)           12352     \n",
            "                                                                 \n",
            " max_pooling1d_4 (MaxPoolin  (None, 123, 64)           0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_6 (Conv1D)           (None, 121, 128)          24704     \n",
            "                                                                 \n",
            " max_pooling1d_5 (MaxPoolin  (None, 60, 128)           0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " lstm_9 (LSTM)               (None, 128)               131584    \n",
            "                                                                 \n",
            " flatten_8 (Flatten)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_16 (Dense)            (None, 256)               33024     \n",
            "                                                                 \n",
            " dense_17 (Dense)            (None, 1)                 257       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1531137 (5.84 MB)\n",
            "Trainable params: 1531137 (5.84 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#compile the model\n",
        "model_clstm2.compile(\n",
        "    optimizer= 'adam',\n",
        "    loss= 'binary_crossentropy',\n",
        "    metrics=['accuracy'],\n",
        ")\n",
        "\n",
        "#model training\n",
        "his_clstm2 = model_clstm2.fit(x_train, Y_train, epochs=15, batch_size=128, callbacks=[callback],\n",
        "validation_data=(x_val, Y_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJ05-tskzkvN",
        "outputId": "2576ae48-849e-46f0-88b4-1661276a5b3f"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "157/157 [==============================] - 22s 111ms/step - loss: 0.4411 - accuracy: 0.7755 - val_loss: 0.2982 - val_accuracy: 0.8744\n",
            "Epoch 2/15\n",
            "157/157 [==============================] - 11s 71ms/step - loss: 0.2304 - accuracy: 0.9071 - val_loss: 0.2900 - val_accuracy: 0.8890\n",
            "Epoch 3/15\n",
            "157/157 [==============================] - 9s 60ms/step - loss: 0.1468 - accuracy: 0.9470 - val_loss: 0.3270 - val_accuracy: 0.8760\n",
            "Epoch 4/15\n",
            "157/157 [==============================] - 7s 47ms/step - loss: 0.0915 - accuracy: 0.9668 - val_loss: 0.4111 - val_accuracy: 0.8772\n",
            "Epoch 5/15\n",
            "157/157 [==============================] - 6s 37ms/step - loss: 0.0450 - accuracy: 0.9840 - val_loss: 0.4931 - val_accuracy: 0.8550\n",
            "Epoch 6/15\n",
            "157/157 [==============================] - 6s 39ms/step - loss: 0.0292 - accuracy: 0.9901 - val_loss: 0.5837 - val_accuracy: 0.8748\n",
            "Epoch 7/15\n",
            "157/157 [==============================] - 5s 34ms/step - loss: 0.0290 - accuracy: 0.9894 - val_loss: 0.5457 - val_accuracy: 0.8860\n",
            "Epoch 8/15\n",
            "157/157 [==============================] - 4s 27ms/step - loss: 0.0189 - accuracy: 0.9931 - val_loss: 0.6558 - val_accuracy: 0.8776\n",
            "Epoch 9/15\n",
            "157/157 [==============================] - 5s 31ms/step - loss: 0.0127 - accuracy: 0.9955 - val_loss: 0.8309 - val_accuracy: 0.8668\n",
            "Epoch 10/15\n",
            "157/157 [==============================] - 4s 26ms/step - loss: 0.0229 - accuracy: 0.9919 - val_loss: 0.8199 - val_accuracy: 0.8450\n",
            "Epoch 11/15\n",
            "157/157 [==============================] - 5s 31ms/step - loss: 0.0199 - accuracy: 0.9933 - val_loss: 1.0425 - val_accuracy: 0.8384\n",
            "Epoch 12/15\n",
            "157/157 [==============================] - 4s 28ms/step - loss: 0.0109 - accuracy: 0.9962 - val_loss: 0.8489 - val_accuracy: 0.8808\n",
            "Epoch 13/15\n",
            "157/157 [==============================] - 4s 26ms/step - loss: 0.0159 - accuracy: 0.9948 - val_loss: 0.7573 - val_accuracy: 0.8746\n",
            "Epoch 14/15\n",
            "157/157 [==============================] - 4s 29ms/step - loss: 0.0054 - accuracy: 0.9980 - val_loss: 1.0378 - val_accuracy: 0.8712\n",
            "Epoch 15/15\n",
            "157/157 [==============================] - 4s 24ms/step - loss: 0.0059 - accuracy: 0.9981 - val_loss: 0.9491 - val_accuracy: 0.8730\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#evaluate on test set\n",
        "evaluation = model_clstm2.evaluate(X_test, y_test)\n",
        "for i in range(len(evaluation)):\n",
        "  print(f'{model.metrics_names[i]} ---> {evaluation[i]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "URKBr3kl1Iq7",
        "outputId": "29317775-7115-414d-a78f-1f573c90d984"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 3s 4ms/step - loss: 1.0001 - accuracy: 0.8635\n",
            "loss ---> 1.000058650970459\n",
            "accuracy ---> 0.8634799718856812\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#stacking 3 Conv1D layers and one BiLSTM layer\n",
        "model_clstm3 = Sequential()\n",
        "model_clstm3.add(Embedding(5000, 256, input_length=max_review_length))\n",
        "model_clstm3.add(Conv1D(64, 3, activation='relu'))\n",
        "model_clstm3.add(MaxPooling1D(2))\n",
        "model_clstm3.add(Conv1D(64, 3, activation='relu'))\n",
        "model_clstm3.add(MaxPooling1D(2))\n",
        "model_clstm3.add(Conv1D(128, 3, activation='relu'))\n",
        "model_clstm3.add(MaxPooling1D(2))\n",
        "model_clstm3.add(Bidirectional(LSTM(128)))\n",
        "\n",
        "model_clstm3.add(Flatten())\n",
        "model_clstm3.add(Dense(128))\n",
        "model_clstm3.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# see the model status now\n",
        "model_clstm3.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_QA6-6c01R3w",
        "outputId": "e97f6bbf-973c-4579-992d-588e29e1b2af"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_16\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_16 (Embedding)    (None, 500, 256)          1280000   \n",
            "                                                                 \n",
            " conv1d_17 (Conv1D)          (None, 498, 64)           49216     \n",
            "                                                                 \n",
            " max_pooling1d_16 (MaxPooli  (None, 249, 64)           0         \n",
            " ng1D)                                                           \n",
            "                                                                 \n",
            " conv1d_18 (Conv1D)          (None, 247, 64)           12352     \n",
            "                                                                 \n",
            " max_pooling1d_17 (MaxPooli  (None, 123, 64)           0         \n",
            " ng1D)                                                           \n",
            "                                                                 \n",
            " conv1d_19 (Conv1D)          (None, 121, 128)          24704     \n",
            "                                                                 \n",
            " max_pooling1d_18 (MaxPooli  (None, 60, 128)           0         \n",
            " ng1D)                                                           \n",
            "                                                                 \n",
            " bidirectional_5 (Bidirecti  (None, 256)               263168    \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " flatten_13 (Flatten)        (None, 256)               0         \n",
            "                                                                 \n",
            " dense_26 (Dense)            (None, 128)               32896     \n",
            "                                                                 \n",
            " dense_27 (Dense)            (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1662465 (6.34 MB)\n",
            "Trainable params: 1662465 (6.34 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#compile the model\n",
        "model_clstm3.compile(\n",
        "    optimizer= 'adam',\n",
        "    loss= 'binary_crossentropy',\n",
        "    metrics=['accuracy'],\n",
        ")\n",
        "\n",
        "#model training\n",
        "his_clstm3 = model_clstm3.fit(x_train, Y_train, epochs=15, batch_size=128, callbacks=[callback],\n",
        "validation_data=(x_val, Y_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQVNtt5D2bFl",
        "outputId": "e8eecad6-a7c5-4a75-8786-5fefcc957f12"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "157/157 [==============================] - 25s 122ms/step - loss: 0.4456 - accuracy: 0.7743 - val_loss: 0.2998 - val_accuracy: 0.8762\n",
            "Epoch 2/15\n",
            "157/157 [==============================] - 12s 77ms/step - loss: 0.2287 - accuracy: 0.9104 - val_loss: 0.2865 - val_accuracy: 0.8852\n",
            "Epoch 3/15\n",
            "157/157 [==============================] - 9s 61ms/step - loss: 0.1439 - accuracy: 0.9495 - val_loss: 0.3145 - val_accuracy: 0.8848\n",
            "Epoch 4/15\n",
            "157/157 [==============================] - 8s 52ms/step - loss: 0.0761 - accuracy: 0.9744 - val_loss: 0.4696 - val_accuracy: 0.8790\n",
            "Epoch 5/15\n",
            "157/157 [==============================] - 7s 41ms/step - loss: 0.0472 - accuracy: 0.9829 - val_loss: 0.4583 - val_accuracy: 0.8660\n",
            "Epoch 6/15\n",
            "157/157 [==============================] - 7s 45ms/step - loss: 0.0277 - accuracy: 0.9906 - val_loss: 0.6794 - val_accuracy: 0.8520\n",
            "Epoch 7/15\n",
            "157/157 [==============================] - 6s 39ms/step - loss: 0.0330 - accuracy: 0.9880 - val_loss: 0.6575 - val_accuracy: 0.8774\n",
            "Epoch 8/15\n",
            "157/157 [==============================] - 6s 36ms/step - loss: 0.0082 - accuracy: 0.9976 - val_loss: 0.7732 - val_accuracy: 0.8620\n",
            "Epoch 9/15\n",
            "157/157 [==============================] - 5s 34ms/step - loss: 0.0057 - accuracy: 0.9982 - val_loss: 0.8722 - val_accuracy: 0.8690\n",
            "Epoch 10/15\n",
            "157/157 [==============================] - 5s 35ms/step - loss: 0.0604 - accuracy: 0.9804 - val_loss: 0.6409 - val_accuracy: 0.8800\n",
            "Epoch 11/15\n",
            "157/157 [==============================] - 6s 36ms/step - loss: 0.0055 - accuracy: 0.9986 - val_loss: 0.8298 - val_accuracy: 0.8760\n",
            "Epoch 12/15\n",
            "157/157 [==============================] - 5s 35ms/step - loss: 0.0035 - accuracy: 0.9989 - val_loss: 1.5305 - val_accuracy: 0.8202\n",
            "Epoch 13/15\n",
            "157/157 [==============================] - 5s 34ms/step - loss: 0.0522 - accuracy: 0.9816 - val_loss: 0.7260 - val_accuracy: 0.8696\n",
            "Epoch 14/15\n",
            "157/157 [==============================] - 5s 34ms/step - loss: 0.0034 - accuracy: 0.9992 - val_loss: 0.8072 - val_accuracy: 0.8802\n",
            "Epoch 15/15\n",
            "157/157 [==============================] - 5s 32ms/step - loss: 3.5524e-04 - accuracy: 1.0000 - val_loss: 0.9434 - val_accuracy: 0.8820\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#evaluate on test set\n",
        "evaluation = model_clstm3.evaluate(X_test, y_test)\n",
        "for i in range(len(evaluation)):\n",
        "  print(f'{model.metrics_names[i]} ---> {evaluation[i]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9lVlJfs3EhI",
        "outputId": "3126f7ab-d457-4d76-f61c-4c046e047cf1"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 4s 5ms/step - loss: 1.0317 - accuracy: 0.8666\n",
            "loss ---> 1.0317059755325317\n",
            "accuracy ---> 0.8666399717330933\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGpoipyLPj4h"
      },
      "source": [
        "# Wrap up\n",
        "\n",
        "Report the accuracies you got from different architectures and write down any insights you have learned."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "2j2vYX2U5J43"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Convolution BiLSTM outperformed other models. This model had an accuracy of 86.7% on the test data. This is followed by the Convolution LSTM model, with an accuracy of 86.3%. The next better performing model is the CNN model, with an accuracy of 86.1%. However, the LSTM and BiLSTM models had the leact performance, with an accuracy of 85.3% and 84.3% respectively on the test data.\n",
        "\n",
        "Hence, from this dataset and model architectures, I can deduce that the CNN model has a better performance on the test data. Also, increasing the number of CNN layers also helps improve the model performance, as can be seen in the Conv1D and ConvLSTM and ConvBiLSTM models.\n"
      ],
      "metadata": {
        "id": "DYaO5-5q9f-x"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "an0aCQzXPj4h"
      },
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}